{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 2\n",
    "Due on Monday, Oct 30, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Animesh Anant Sharma, aas2325\n",
    "\n",
    "Member 2: Himanshu Aggarwal, ha2467\n",
    "\n",
    "Member 3: Name, UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "from __future__ import divisio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "num_steps=10001\n",
    "n_classes=10\n",
    "dropout = 0.75\n",
    "LOGDIR = \"/tmp/cifar10/q2_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer_reg(input, size_in, size_out,name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        pre_activation = tf.nn.bias_add(conv, b)\n",
    "        act = tf.nn.relu(pre_activation)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act, tf.nn.l2_loss(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer_dropout(input, size_in, size_out,name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        pre_activation = tf.nn.bias_add(conv, b)\n",
    "        act = tf.nn.relu(pre_activation)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act, tf.nn.l2_loss(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer_reg(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act, tf.nn.l2_loss(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer_dropout(input, size_in, size_out, keep_prob, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        act_drop=tf.nn.dropout(act,keep_prob)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act_drop, tf.nn.l2_loss(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(X,y, batchsize):\n",
    "    indices=np.random.choice(X.shape[0], batchsize, replace=False)\n",
    "    X_batch=X[indices,:,:,:]\n",
    "    y_batch=y[:,indices]\n",
    "    return X_batch,y_batch.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_enhanced(num_conv,num_fc, X_complete, y_complete, X_test):\n",
    "    \n",
    "    X_training=X_complete[:45000,:,:,:]\n",
    "    X_testing=X_complete[49000:,:,:,:]\n",
    "    Y_training=y_complete[:,:45000]\n",
    "    Y_testing=y_complete[:,49000:]\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate= tf.placeholder(tf.float32)\n",
    "    x=tf.placeholder(tf.float32,shape=[None,32,32,3],name=\"x\")\n",
    "    y=tf.placeholder(tf.int32,shape=[None,10],name=\"labels\")\n",
    "    conv1, l2_conv1 = conv_layer_reg(x, 3, 32, \"conv1\")\n",
    "    pool1=tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\",name=\"pool1\")\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm1')\n",
    "    regularizer=l2_conv1\n",
    "    conv_out,l2_conv2 = conv_layer_reg(norm1, 32, 32, \"conv2\")\n",
    "    regularizer=regularizer+l2_conv2\n",
    "    norm2 = tf.nn.lrn(conv_out, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    pool2=tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\",name=\"pool2\")\n",
    "    \n",
    "    flattened=tf.reshape(pool2,[-1,8*8*32])\n",
    "    \n",
    "    fc1, l2_fc_1 = fc_layer_dropout(flattened, 8 * 8 * 32, 256, keep_prob, \"fc1\")\n",
    "    relu = tf.nn.relu(fc1)\n",
    "    regularizer=regularizer+l2_fc_1\n",
    "    embedding_input = relu\n",
    "    tf.summary.histogram(\"fc1/relu\", relu)\n",
    "    embedding_size = 256\n",
    "    logits, l2_fc_2 = fc_layer_reg(relu, 256, 10, \"fc2\")\n",
    "    regularizer=regularizer+l2_fc_2\n",
    "    \n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)+0.01*regularizer, name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            # Get the gradient pairs (Tensor, Variable)\n",
    "            grads = optimizer.compute_gradients(xent)\n",
    "            # Update the weights wrt to the gradient\n",
    "            train_step = optimizer.apply_gradients(grads)\n",
    "            # Save the grads with tf.summary.histogram\n",
    "            for index, grad in enumerate(grads):\n",
    "                tf.summary.histogram(\"{}-grad\".format(grads[index][1].name.replace(':','_')), grads[index])\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        \n",
    "    summ = tf.summary.merge_all()\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(LOGDIR + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(LOGDIR + '/test', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    rate=0.002\n",
    "    drop_prob=0.8\n",
    "    \n",
    "    \n",
    "    for i in range(0,num_steps):\n",
    "        X_batch, Y_batch=getBatch(X_training,Y_training,128)\n",
    "        \n",
    "        if i%3000==0:\n",
    "            rate/=2\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            [train_accuracy, s, xent_e] = sess.run([accuracy, summ,xent], feed_dict={learning_rate:rate,keep_prob:1,x: X_batch, y: Y_batch})\n",
    "            train_writer.add_summary(s, i)\n",
    "            print (\"Step {} Train accuracy {} Training Loss {} \").format(i, train_accuracy, xent_e)\n",
    "            [test_accuracy,sum]= sess.run([accuracy, summ], feed_dict={learning_rate:rate, keep_prob:1,x: X_testing, y: Y_testing.T})\n",
    "            test_writer.add_summary(sum, i)\n",
    "            print (\"Step {} Validation set accuracy {}\").format(i,test_accuracy)\n",
    "            \n",
    "        sess.run(train_step,feed_dict={learning_rate:rate,keep_prob:drop_prob,x:X_batch,y:Y_batch})\n",
    "    \n",
    "    logits_final=sess.run([logits],feed_dict={learning_rate:rate,keep_prob:1,x:X_test,y:Y_batch})\n",
    "    print(logits.shape)\n",
    "    save_predictions(\"/Users/himanshuaggarwal/Desktop/hw2_output/q2_logits.npy\",logits_final.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    a=range(0,y.shape[0])\n",
    "    y_one_hot[a,y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'horse': 7, 'automobile': 1, 'deer': 4, 'dog': 5, 'frog': 6, 'cat': 3, 'truck': 9, 'ship': 8, 'airplane': 0, 'bird': 2}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "(3072, 50000)\n",
      "(50000,)\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = '/Users/himanshuaggarwal/Desktop/deep_learning_projects/cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X = X.T\n",
    "\n",
    "\n",
    "#training set\n",
    "X_train = X[:45000]\n",
    "#mean of training dataset\n",
    "me = np.mean(X_train, axis = 0)\n",
    "#std of training dataset\n",
    "st = np.std(X_train, axis = 0)\n",
    "X -= me\n",
    "X /= st\n",
    "y=one_hot(y_train,10)\n",
    "X= np.reshape(X, [-1, 32, 32, 3])\n",
    "\n",
    "\n",
    "#test dataset\n",
    "X_test = X_test.T\n",
    "#test set normalization\n",
    "X_test -= me\n",
    "X_test /= st\n",
    "X_test= np.reshape(X_test, [-1, 32, 32, 3])\n",
    "print(X_test.shape)\n",
    "y=y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Train accuracy 0.1015625 Training Loss 29.4365444183 \n",
      "Step 0 Validation set accuracy 0.119999997318\n",
      "Step 500 Train accuracy 0.5546875 Training Loss 3.08751392365 \n",
      "Step 500 Validation set accuracy 0.550999999046\n",
      "Step 1000 Train accuracy 0.7109375 Training Loss 1.56705629826 \n",
      "Step 1000 Validation set accuracy 0.652999997139\n",
      "Step 1500 Train accuracy 0.6015625 Training Loss 1.46230316162 \n",
      "Step 1500 Validation set accuracy 0.638000011444\n",
      "Step 2000 Train accuracy 0.671875 Training Loss 1.34932303429 \n",
      "Step 2000 Validation set accuracy 0.660000026226\n",
      "Step 2500 Train accuracy 0.6953125 Training Loss 1.29681134224 \n",
      "Step 2500 Validation set accuracy 0.685000002384\n",
      "Step 3000 Train accuracy 0.734375 Training Loss 1.15503358841 \n",
      "Step 3000 Validation set accuracy 0.68599998951\n",
      "Step 3500 Train accuracy 0.703125 Training Loss 1.21635103226 \n",
      "Step 3500 Validation set accuracy 0.713999986649\n",
      "Step 4000 Train accuracy 0.8046875 Training Loss 1.04057741165 \n",
      "Step 4000 Validation set accuracy 0.689999997616\n",
      "Step 4500 Train accuracy 0.7578125 Training Loss 1.00886631012 \n",
      "Step 4500 Validation set accuracy 0.721000015736\n",
      "Step 5000 Train accuracy 0.7578125 Training Loss 1.07857227325 \n",
      "Step 5000 Validation set accuracy 0.725000023842\n",
      "Step 5500 Train accuracy 0.7265625 Training Loss 1.17000412941 \n",
      "Step 5500 Validation set accuracy 0.740999996662\n",
      "Step 6000 Train accuracy 0.796875 Training Loss 1.03544795513 \n",
      "Step 6000 Validation set accuracy 0.721000015736\n",
      "Step 6500 Train accuracy 0.8203125 Training Loss 0.987402021885 \n",
      "Step 6500 Validation set accuracy 0.753000020981\n",
      "Step 7000 Train accuracy 0.78125 Training Loss 1.034719944 \n",
      "Step 7000 Validation set accuracy 0.731000006199\n",
      "Step 7500 Train accuracy 0.8359375 Training Loss 0.998201727867 \n",
      "Step 7500 Validation set accuracy 0.736000001431\n",
      "Step 8000 Train accuracy 0.796875 Training Loss 0.968656420708 \n",
      "Step 8000 Validation set accuracy 0.757000029087\n",
      "Step 8500 Train accuracy 0.78125 Training Loss 0.981494367123 \n",
      "Step 8500 Validation set accuracy 0.750999987125\n",
      "Step 9000 Train accuracy 0.8046875 Training Loss 0.98321634531 \n",
      "Step 9000 Validation set accuracy 0.748000025749\n",
      "Step 9500 Train accuracy 0.7421875 Training Loss 1.06074094772 \n",
      "Step 9500 Validation set accuracy 0.76599997282\n",
      "Step 10000 Train accuracy 0.7890625 Training Loss 0.915993213654 \n",
      "Step 10000 Validation set accuracy 0.762000024319\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "cifar10_enhanced(2,2,X,y,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load(\"/Users/himanshuaggarwal/Desktop/hw2_output/q2_logits.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
